{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlatDrake2875/hack_sie_2024/blob/main/Hackathon_SIE_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Masked Face Recognition model"
      ],
      "metadata": {
        "id": "qA9M1gpwU9ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install matplotlib numpy opencv-python pillow\n",
        "!pip install pyyaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8k-YbYvGObY",
        "outputId": "dc73c051-4f36-4a36-8f37-7b4d99fde52d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4NzakTc-2tqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Google Colab"
      ],
      "metadata": {
        "id": "_Cd3JIjqU6O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmCjeNWdGYnJ",
        "outputId": "f78c4f56-c812-442e-e20e-477aa4abd149"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "hack_sie_path = '/content/drive/My Drive/Hack_SIE_2024'\n",
        "os.listdir(hack_sie_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-zXQ9mpU8bG",
        "outputId": "c92f9684-d499-4ccf-c747-e200e7b07053"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ReadME.MD', 'timer.py', 'db', 'hackaton_wild_ds', 'test', 'Modified', 'YOLO']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesare de Date"
      ],
      "metadata": {
        "id": "gMR9NFllsSE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "input_folder = os.path.join(hack_sie_path, 'db')\n",
        "output_folder = os.path.join(hack_sie_path, 'Modified')\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "person_folders = os.listdir(input_folder)\n",
        "print(person_folders)\n",
        "\n",
        "for person_id, person_folder in enumerate(person_folders, start=1):\n",
        "    person_path = os.path.join(input_folder, person_folder)\n",
        "\n",
        "    if os.path.isdir(person_path):\n",
        "        images = os.listdir(person_path)\n",
        "\n",
        "        for image in images:\n",
        "            if 'Indoor' in image and 'M' in image:\n",
        "                new_name = f\"{person_id}_IM.jpg\"\n",
        "            elif 'Indoor' in image:\n",
        "                new_name = f\"{person_id}_I.jpg\"\n",
        "            elif 'Outdoor' in image and 'M' in image:\n",
        "                new_name = f\"{person_id}_OM.jpg\"\n",
        "            elif 'Outdoor' in image:\n",
        "                new_name = f\"{person_id}_O.jpg\"\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            old_file_path = os.path.join(person_path, image)\n",
        "            new_file_path = os.path.join(output_folder, new_name)\n",
        "\n",
        "            shutil.copy2(old_file_path, new_file_path)\n",
        "\n",
        "# print(\"Pozele au fost redenumite și salvate în folderul modificat.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leT-KKz7sJGP",
        "outputId": "9ffff5f9-6925-4156-de8f-89890299b68f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Andres', 'Ester', 'DaniF', 'Isa', 'Dani B', 'Cristina', 'Diego', 'Marcos', 'Narciso', 'Pablo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Antrenarea"
      ],
      "metadata": {
        "id": "X7GuglT3sUwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_weights_path = os.path.join(hack_sie_path, 'YOLO', 'Yolov7')\n",
        "\n",
        "os.makedirs(yolo_weights_path, exist_ok=True)\n",
        "\n",
        "!wget -O \"{yolo_weights_path}/yolov7.pt\" https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n",
        "\n",
        "print(\"Contents of YOLOv7 weights directory:\", os.listdir(yolo_weights_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSIj3w3l20KN",
        "outputId": "5f3a64da-fb4a-4560-d8fd-c029e81ea939"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-25 14:39:06--  https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/b0243edf-9fb0-4337-95e1-42555f1b37cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241025%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241025T143906Z&X-Amz-Expires=300&X-Amz-Signature=6748c90209b7fb481cadd0905cb658e857b8e8f949946221d3c04a4238f5931c&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov7.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-10-25 14:39:06--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/b0243edf-9fb0-4337-95e1-42555f1b37cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241025%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241025T143906Z&X-Amz-Expires=300&X-Amz-Signature=6748c90209b7fb481cadd0905cb658e857b8e8f949946221d3c04a4238f5931c&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov7.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75587165 (72M) [application/octet-stream]\n",
            "Saving to: ‘/content/drive/My Drive/Hack_SIE_2024/YOLO/Yolov7/yolov7.pt’\n",
            "\n",
            "/content/drive/My D 100%[===================>]  72.08M  75.7MB/s    in 1.0s    \n",
            "\n",
            "2024-10-25 14:39:07 (75.7 MB/s) - ‘/content/drive/My Drive/Hack_SIE_2024/YOLO/Yolov7/yolov7.pt’ saved [75587165/75587165]\n",
            "\n",
            "Contents of YOLOv7 weights directory: ['yolov7.pt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image  # Import PIL to convert NumPy arrays to PIL Images\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set the device to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define base path\n",
        "hack_sie_path = '/content/drive/My Drive/Hack_SIE_2024'\n",
        "\n",
        "# Path configurations\n",
        "train_dir = os.path.join(hack_sie_path, 'Modified')\n",
        "model_save_path = os.path.join(hack_sie_path, 'NN', 'v1.pth')\n",
        "\n",
        "# Load YOLOv7 for face detection\n",
        "yolo_model = torch.hub.load('WongKinYiu/yolov7', 'custom',\n",
        "                            os.path.join(hack_sie_path, 'YOLO', 'Yolov7', 'yolov7.pt'),\n",
        "                            source='github', trust_repo=True).to(device).eval()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTA_1oE0FbU2",
        "outputId": "d4a6a34f-7465-4f82-91b4-e48532920150"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/WongKinYiu_yolov7_main\n",
            "/root/.cache/torch/hub/WongKinYiu_yolov7_main/hubconf.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(path_or_model, map_location=torch.device('cpu')) if isinstance(path_or_model, str) else path_or_model  # load checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding autoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data augumentation"
      ],
      "metadata": {
        "id": "zJDLFgxGFgnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "gGjhgYivFc98"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_image(image, detections=None):\n",
        "    \"\"\"Display image with bounding boxes for debugging.\"\"\"\n",
        "    # Create a figure\n",
        "    plt.figure(figsize=(4, 4))  # Adjust the figure size if needed\n",
        "\n",
        "    if detections is not None:\n",
        "        for det in detections:\n",
        "            x1, y1, x2, y2, conf, cls = det\n",
        "            # Draw bounding boxes on the image\n",
        "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "\n",
        "    # Display image with matplotlib (cv2 image uses BGR, need to convert to RGB)\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')  # Remove axes for better visualization\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Prepare dataset by scanning the directory\n",
        "        for filename in os.listdir(image_dir):\n",
        "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "                # Extract person ID from filename\n",
        "                person_id = int(filename.split('_')[0])\n",
        "                self.image_paths.append(os.path.join(image_dir, filename))\n",
        "                self.labels.append(person_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load image with OpenCV\n",
        "        image = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB as YOLOv7 expects RGB\n",
        "\n",
        "        # Detect faces using YOLOv7\n",
        "        results = yolo_model(image_rgb)\n",
        "\n",
        "        # Get the detections\n",
        "        detections = results.xyxy[0]  # Get the first (and only) image detections\n",
        "        if len(detections) == 0:\n",
        "            # If no face detected, return a zero tensor\n",
        "            face_img = torch.zeros((3, 224, 224))\n",
        "        else:\n",
        "            # Assume the first detected face is the target\n",
        "            x1, y1, x2, y2, conf, cls = detections[0]\n",
        "            face_img = image[int(y1):int(y2), int(x1):int(x2)]  # Crop the face\n",
        "\n",
        "            # Debug this line to display the cropped face with bounding box\n",
        "            # display_image(image, detections)\n",
        "\n",
        "            # Convert the cropped face to a PIL Image\n",
        "            face_img = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "            # Apply transformations\n",
        "            if self.transform:\n",
        "                face_img = self.transform(face_img)\n",
        "\n",
        "        return face_img, torch.tensor(label - 1)  # Adjust label to start from 0\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = FaceDataset(train_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "fiMnhnIGFf6u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Define the model (using ResNet for classification)\n",
        "model = models.resnet18(weights='DEFAULT')  # Use 'DEFAULT' for newer PyTorch versions\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 10)  # Adjust the number of classes as needed\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print epoch statistics\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "# torch.save(model.state_dict(), model_save_path)\n",
        "# print(f'Model saved to {model_save_path}')"
      ],
      "metadata": {
        "id": "aUQsiqf-sFN-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}